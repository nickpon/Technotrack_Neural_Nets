{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение, для реализации \"Ванильной\" RNN\n",
    "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3,3))*3\n",
    "b = torch.ones((3,3))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 5., 5.],\n",
       "        [5., 5., 5.],\n",
       "        [5., 5., 5.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 45., 45.],\n",
       "        [45., 45., 45.],\n",
       "        [45., 45., 45.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.],\n",
       "        [15., 15., 15.],\n",
       "        [15., 15., 15.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'ololoasdasddqweqw123456789'\n",
    "# word = 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=8, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 200\n",
    "optim     = SGD(rnn.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.36385345458984\n",
      "Clip gradient :  6.86864607027851\n",
      "52.808048248291016\n",
      "Clip gradient :  18.76307334055694\n",
      "35.8555793762207\n",
      "Clip gradient :  10.531217057028805\n",
      "18.26323699951172\n",
      "Clip gradient :  6.579962303244384\n",
      "5.721493244171143\n",
      "Clip gradient :  7.209886694543383\n",
      "3.5520081520080566\n",
      "Clip gradient :  4.494360920223783\n",
      "2.1401476860046387\n",
      "Clip gradient :  3.101429260895991\n",
      "4.411831855773926\n",
      "Clip gradient :  10.557477043117581\n",
      "6.175124645233154\n",
      "Clip gradient :  22.936552644124166\n",
      "2.303819179534912\n",
      "Clip gradient :  1.3037154058705873\n",
      "1.2081799507141113\n",
      "Clip gradient :  7.820986150586707\n",
      "1.9899497032165527\n",
      "Clip gradient :  1.0529065038256773\n",
      "1.7811946868896484\n",
      "Clip gradient :  1.485835834794035\n",
      "1.6265811920166016\n",
      "Clip gradient :  1.1076671744673605\n",
      "1.5290412902832031\n",
      "Clip gradient :  0.7525557152875515\n",
      "1.4064817428588867\n",
      "Clip gradient :  0.6585222868922677\n",
      "0.26937103271484375\n",
      "Clip gradient :  0.33546065151328947\n",
      "0.13422203063964844\n",
      "Clip gradient :  0.1849632183589549\n",
      "0.09629058837890625\n",
      "Clip gradient :  0.07863002441880117\n",
      "0.07278156280517578\n",
      "Clip gradient :  0.042655780828987494\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово\n",
    "Сохранить ноутбук с предсказанием и пройденным assert и прислать на почту a.murashev@corp.mail.ru\n",
    "c темой:\n",
    "\n",
    "\n",
    "[МФТИ\\_2019\\_1] ДЗ №8 ФИО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Написать реализацию LSTM и обучить предсказывать слово\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(LSTM, self).__init__() \n",
    "        \n",
    "        self.first_x       = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.first_hidden  = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.first_sigm    = nn.Sigmoid()\n",
    "        \n",
    "        self.second_x      = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.second_hidden = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.second_sigm   = nn.Sigmoid()\n",
    "        \n",
    "        self.third_x       = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.third_hidden  = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.third_tanh    = nn.Tanh()\n",
    "        \n",
    "        self.fourth_x      = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.fourth_hidden = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.fourth_sigm   = nn.Sigmoid()\n",
    "        \n",
    "        self.tanh          = nn.Tanh()\n",
    "        self.outweight     = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden, prev_C):\n",
    "        first              = self.first_sigm(self.first_x(x) + self.first_hidden(prev_hidden))\n",
    "        second             = self.second_sigm(self.second_x(x) + self.second_hidden(prev_hidden))\n",
    "        third              = self.third_tanh(self.third_x(x) + self.third_hidden(prev_hidden))\n",
    "        fourth             = self.fourth_sigm(self.fourth_x(x) + self.fourth_hidden(prev_hidden))\n",
    "        \n",
    "        C                  = first * prev_C + second * third\n",
    "        hh                 = fourth * self.tanh(C)\n",
    "        output             = self.outweight(hh)\n",
    "        \n",
    "        return output, hh, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(net, is_CLIP_GRAD=True, e_cnt=100, hid_s=3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim     = SGD(net.parameters(), lr = 0.1, momentum=0.9)\n",
    "    \n",
    "    CLIP_GRAD = is_CLIP_GRAD\n",
    "\n",
    "    for epoch in range(e_cnt):\n",
    "        hh = torch.zeros(hid_s)\n",
    "        C = torch.zeros(hid_s)\n",
    "        loss = 0\n",
    "        optim.zero_grad()\n",
    "        for sample, next_sample in ds:\n",
    "            x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "            target =  torch.LongTensor([next_sample])\n",
    "    \n",
    "            y, hh, C = net(x, hh, C)\n",
    "            \n",
    "            loss += criterion(y, target)\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print (loss.data.item())\n",
    "            if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=5))\n",
    "        else: \n",
    "            if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "        \n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lstm(net, hid_s=3):\n",
    "    net.eval()\n",
    "    hh = torch.zeros(hid_s)\n",
    "    C = torch.zeros(hid_s)\n",
    "    id = 0\n",
    "    softmax  = nn.Softmax(dim=1)\n",
    "    predword = ds.get_char_by_id(id)\n",
    "    for c in enumerate(word[:-1]):\n",
    "        x = ds.get_one_hot(id).unsqueeze(0)\n",
    "        y, hh, C = net(x, hh, C)\n",
    "        y = softmax(y)\n",
    "        m, id = torch.max(y, 1)\n",
    "        id = id.data[0]\n",
    "        predword += ds.get_char_by_id(id)\n",
    "    print ('Prediction:\\t' , predword)\n",
    "    print(\"Original:\\t\", word)\n",
    "    assert(predword == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "hid_s = 5\n",
    "lstm = LSTM(in_size=ds.vec_size, hidden_size=hid_s, out_size=ds.vec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.03343963623047\n",
      "Clip gradient :  4.185040428649836\n",
      "26.815519332885742\n",
      "Clip gradient :  12.007424742641563\n",
      "13.877161026000977\n",
      "Clip gradient :  2.108402869076971\n",
      "3.4952526092529297\n",
      "Clip gradient :  4.598913211104315\n",
      "1.6169724464416504\n",
      "Clip gradient :  1.7350150708633005\n",
      "0.5218076705932617\n",
      "Clip gradient :  0.3565007929432089\n",
      "0.25933170318603516\n",
      "Clip gradient :  0.05049733697658246\n",
      "0.16981220245361328\n",
      "Clip gradient :  0.03405291813887915\n",
      "0.12395763397216797\n",
      "Clip gradient :  0.025201981814414767\n",
      "0.0975809097290039\n",
      "Clip gradient :  0.019990398241705398\n",
      "0.08038997650146484\n",
      "Clip gradient :  0.01653678048972525\n",
      "0.06836795806884766\n",
      "Clip gradient :  0.014091730677699846\n",
      "0.059477806091308594\n",
      "Clip gradient :  0.012279191169092148\n",
      "0.05263805389404297\n",
      "Clip gradient :  0.010879666357407626\n",
      "0.047209739685058594\n",
      "Clip gradient :  0.009766171411975829\n",
      "0.042799949645996094\n",
      "Clip gradient :  0.008860466554641292\n",
      "0.03914451599121094\n",
      "Clip gradient :  0.008108552160995104\n",
      "0.036060333251953125\n",
      "Clip gradient :  0.00747308009018271\n",
      "0.033432960510253906\n",
      "Clip gradient :  0.006931976957205789\n",
      "0.031165122985839844\n",
      "Clip gradient :  0.006465429413582028\n"
     ]
    }
   ],
   "source": [
    "train_lstm(net=lstm, is_CLIP_GRAD=True, e_cnt=1000, hid_s=hid_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "test_lstm(net=lstm, hid_s=hid_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Написать реализацию GRU и обучить предсказывать слово\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(GRU, self).__init__()   \n",
    "        \n",
    "        self.first_x       = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.first_hidden  = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.first_sigm    = nn.Sigmoid()\n",
    "        \n",
    "        self.second_x      = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.second_hidden = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.second_sigm   = nn.Sigmoid()\n",
    "        \n",
    "        self.third_x       = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.third_hidden  = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.third_tanh    = nn.Tanh()\n",
    "        \n",
    "        self.outweight     = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        first = self.first_sigm(self.first_x(x) + self.first_hidden(prev_hidden))\n",
    "        second = self.second_sigm(self.second_x(x) + self.second_hidden(prev_hidden))\n",
    "        \n",
    "        hh = prev_hidden - first * prev_hidden + first *self.third_tanh(self.third_x(x) +\n",
    "                                                                        self.third_hidden(second * prev_hidden))\n",
    "        output = self.outweight(hh)\n",
    "        \n",
    "        return output, hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "hid_s = 5\n",
    "gru = GRU(in_size=ds.vec_size, hidden_size=hid_s, out_size=ds.vec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gru(net, is_CLIP_GRAD=True, e_cnt=100, hid_s=3):\n",
    "    CLIP_GRAD = is_CLIP_GRAD\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim     = SGD(net.parameters(), lr = 0.1, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(e_cnt):\n",
    "        hh = torch.zeros(hid_s)\n",
    "        loss = 0\n",
    "        optim.zero_grad()\n",
    "        for sample, next_sample in ds:\n",
    "            x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "            target =  torch.LongTensor([next_sample])\n",
    "    \n",
    "            y, hh = net(x, hh)\n",
    "            \n",
    "            loss += criterion(y, target)\n",
    "         \n",
    "    \n",
    "        loss.backward()\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print (loss.data.item())\n",
    "            if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=5))\n",
    "        else: \n",
    "            if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "        \n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gru(net, hid_s=3):\n",
    "    net.eval()\n",
    "    hh = torch.zeros(hid_s)\n",
    "    id = 0\n",
    "    softmax  = nn.Softmax(dim=1)\n",
    "    predword = ds.get_char_by_id(id)\n",
    "    for c in enumerate(word[:-1]):\n",
    "        x = ds.get_one_hot(id).unsqueeze(0)\n",
    "        y, hh = net(x, hh)\n",
    "        y = softmax(y)\n",
    "        m, id = torch.max(y, 1)\n",
    "        id = id.data[0]\n",
    "        predword += ds.get_char_by_id(id)\n",
    "    print ('Prediction:\\t' , predword)\n",
    "    print(\"Original:\\t\", word)\n",
    "    assert(predword == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.65504455566406\n",
      "Clip gradient :  4.756080706807321\n",
      "5.6750030517578125\n",
      "Clip gradient :  1.1757942029989996\n",
      "1.221165657043457\n",
      "Clip gradient :  8.48200200945489\n",
      "1.4299821853637695\n",
      "Clip gradient :  12.019203308928248\n",
      "0.2422170639038086\n",
      "Clip gradient :  0.2695188530743263\n",
      "0.09823322296142578\n",
      "Clip gradient :  0.0316749506522398\n",
      "0.06674957275390625\n",
      "Clip gradient :  0.019709762150494043\n",
      "0.05107879638671875\n",
      "Clip gradient :  0.015059797172100507\n",
      "0.04145336151123047\n",
      "Clip gradient :  0.012270645496640354\n",
      "0.03486061096191406\n",
      "Clip gradient :  0.010402715270363683\n"
     ]
    }
   ],
   "source": [
    "train_gru(net=gru, is_CLIP_GRAD=True, e_cnt=500, hid_s=hid_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "test_gru(net=gru, hid_s=hid_s)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "technokek",
   "language": "python",
   "name": "technokek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
